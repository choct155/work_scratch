{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Equations and Computer Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first in a set of Notebooks that follow along with the book [*Applied Computational Economics and Finance*](https://mitpress.mit.edu/books/applied-computational-economics-and-finance) (Miranda & Fackler 2004).  The goal is to actually perform some of the exercises and examples that appear througout the book to help solidify my comprehension.  It obviously starts simple, but let's see where we end up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LU Factorization (2.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first example is actually not LU, but forward substitution.  The idea being that it is a completely reasonable method (from a computational standpoint) when we are dealing with a triangular matrix.  Suppose we have a linear model:\n",
    "\n",
    "$$Ax=b$$\n",
    "\n",
    "where the following dimensions apply:\n",
    "\n",
    "+ $A (n \\times p)$\n",
    "+ $x (p \\times 1)$\n",
    "+ $b (n \\times 1)$\n",
    "\n",
    "$A$ and $b$ are given, and $A$ is a lower triangular matrix. \n",
    "\n",
    "$$A=\n",
    "\\begin{bmatrix}\n",
    "a_{11} &         &        &           & 0  \\\\\n",
    "a_{21} & a_{22} &        &           &    \\\\\n",
    "a_{31} & a_{32} & \\ddots &           &    \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\ddots    &    \\\\\n",
    "a_{n1} & a_{n2} & \\ldots & a_{nn-1} & a_{nn}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "How does one determine the solution vector $x$?  Let's try this forward substitution business out.  The algorithm solves for $x$ one position at a time.  It can do so because the equation in the first row only has one unknown.  \n",
    "\n",
    "$$a_{11}x_1=b_1$$\n",
    "\n",
    "The second row equation has two unknowns, but one of them was calculated in the first row.  \n",
    "\n",
    "$$a_{21}x_1 + a_{22}x_2 = b_2$$\n",
    "\n",
    "Each new row introduces only one new unknown.\n",
    "\n",
    "$$x_1 = b_1/a_{11}$$\n",
    "\n",
    "$$x_2=(b_2-a_{21}x_1)/a_{22}$$\n",
    "\n",
    "$$x_3=(b_3-a_{31}x_1-a_{32}x_2)/a_{33}$$\n",
    "\n",
    "$$\\vdots$$\n",
    "\n",
    "$$x_n=(b_n - a_{n1}x_1 - a_{n2}x_2 - \\cdots - a_{n,n-1}x_{n-1})/a_{nn}$$\n",
    "\n",
    "This progression generalizes to the following expression\n",
    "\n",
    "$$x_i = (b_i - \\sum_{j=1}^{i-1} a_{ij} x_j)/a_{ii}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define extent of solution vector\n",
    "n=5\n",
    "\n",
    "#Generate a matrix of random positive integers\n",
    "A=float(rand(1:10,n,n))\n",
    "\n",
    "#Coerce it into a lower triangular by zero sub\n",
    "for i=1:n\n",
    "    for j=1:n\n",
    "        if j>i\n",
    "            A[i,j]=0\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "#Generate a response vector of random positive ints\n",
    "b=float(rand(1:10,n))\n",
    "\n",
    "# Generate array to hold solution\n",
    "# x=[0. for i=1:n]\n",
    "\n",
    "#Define function to solve this system by forward substitution\n",
    "function fsub(A,b)\n",
    "    #Generate array to hold solution\n",
    "    x=[0. for i=1:n]\n",
    "    #Capture the first value\n",
    "    x[1]=b[1]/A[1,1]\n",
    "    #For each equation...\n",
    "    for i=2:length(b)\n",
    "        #...solve for x (returns a 1x1 array)...\n",
    "        tmp=(b[i] - A[i,1:i-1]*x[1:i-1])/A[i,i]\n",
    "        #...and insert into x\n",
    "        x[i]=tmp[1]\n",
    "    end\n",
    "    return x\n",
    "end\n",
    "\n",
    "#Calculate x\n",
    "x=fsub(A,b)\n",
    "\n",
    "#Define function to check the solution vector\n",
    "function csvec(vec1,vec2)\n",
    "    return all([isapprox(vec1[i],vec2[i]) for i in 1:length(b)])\n",
    "end\n",
    "\n",
    "csvec(A*x,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was pretty straightforward (once the syntax issues were ironed out).  The algebraic simplicity of this method makes it attractive from a computational standpoint.  The thing is, most matrices in practice do not share this triangular format.  One way to get them there is to use *LU Factorization* to decompose a matrix into upper and lower triangulars.  Then we can use both back and forward substitution, respectively, to solve for the solution vector $x$.  This approach can be used for all square, nonsingular matrices.\n",
    "\n",
    "There are two phases.  *Factorization* involves the use of Gaussian elimination to factor $A$ into $L$ and $U$.\n",
    "\n",
    "$$A=LU$$\n",
    "\n",
    "Note that $L$ will be row-permuted.  Once factored, we are solving the following system.\n",
    "\n",
    "$$Ax=LUx=L(Ux)=b$$\n",
    "\n",
    "This arrangement permits a straightforward two-step protocol:\n",
    "\n",
    "1. Solve for $y$ in $Ly=b$; and,\n",
    "2. Solve for $x$ in $Ux=y$.\n",
    "\n",
    "First,let's design a function that performs LU factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate a matrix of random positive integers\n",
    "A=float(rand(1:10,n,n))\n",
    "\n",
    "function LU_fact(A)\n",
    "    #Generate containers for LU factors\n",
    "    L=eye(n)\n",
    "    U=copy(A)\n",
    "\n",
    "    #For each column...\n",
    "    for j=1:n\n",
    "        #...and each row below the diagonal...\n",
    "        for i=j+1:n\n",
    "            #...capture the ratio of the diagonal value to lower row value in that column...\n",
    "            tmp_ratio=U[i,j]/U[j,j]\n",
    "            #...store the ratio in L at the lower row value position...\n",
    "            L[i,j]=tmp_ratio\n",
    "            #...and then use the ratio as a multiplier when subtracting the diagonal row from the lower row\n",
    "            #and assign it to the corresponding row in U\n",
    "            U[i,1:n]=U[i,1:n]-tmp_ratio*U[j,1:n]\n",
    "        end\n",
    "    end\n",
    "    return (L,U)\n",
    "end\n",
    "\n",
    "L,U=LU_fact(A)\n",
    "\n",
    "csvec(L*U,A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we solve for $y=Ux$, and ultimately for $x$.  Since $U$ is an upper triangular matrix, we need to use backward substitution in the second stage equation.  Backward substition is just the opposite of forward substitution.  This time, we are operating on an upper triangular matrix.\n",
    "\n",
    "$$A =\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} & \\ldots & a_{1n}  \\\\\n",
    "        & a_{22} & a_{23} & \\ldots & a_{2n}  \\\\\n",
    "        &         & \\ddots  & \\ddots & \\vdots   \\\\\n",
    "        &         &         & \\ddots & a_{n-1,n}\\\\\n",
    "  0     &         &         &        & a_{nn}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "The first equation starts with the last row.\n",
    "\n",
    "$$a_{nn}x_n=b_n$$\n",
    "\n",
    "The second row equation has two unknowns, but one of them was calculated in the first row.  \n",
    "\n",
    "$$a_{n-1,n}x_n + a_{n-1,n-1}x_{n-1} = b_{n-1}$$\n",
    "\n",
    "Each new row introduces only one new unknown.\n",
    "\n",
    "$$x_n = b_n/a_{nn}$$\n",
    "\n",
    "$$x_{n-1}=(b_{n-1}-a_{n-1,n}x_n)/a_{n-1,n-1}$$\n",
    "\n",
    "$$x_{n-2}=(b_{n-2}-a_{n-2,n}x_n-a_{n-2,n-1}x_{n-1})/a_{n-2,n-2}$$\n",
    "\n",
    "$$\\vdots$$\n",
    "\n",
    "$$x_1=(b_1 - a_{1n}x_n - a_{1,n-1}x_{n-1} - \\cdots - a_{12}x_2)/a_{11}$$\n",
    "\n",
    "This progression generalizes to the following expression\n",
    "\n",
    "$$x_i = (b_i - \\sum_{j=i+1}^{n} a_{ij} x_j)/a_{ii}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ly=b:true\n",
      "Ux=y:true\n",
      "Ax=b:true\n"
     ]
    }
   ],
   "source": [
    "#Define function to solve this system by backward substitution\n",
    "function bsub(A,b)\n",
    "    #Capture length of solution vector\n",
    "    nv=size(A)[1]\n",
    "    #Generate array to hold solution\n",
    "    x=[0. for i=1:nv]\n",
    "    #Capture the last value value\n",
    "    x[nv]=b[nv]/A[nv,nv]\n",
    "    #For each equation...\n",
    "    for i=nv-1:-1:1\n",
    "        #...solve for x (returns a 1x1 array)...\n",
    "        tmp=(b[i] - A[i,i+1:nv]*x[i+1:nv])/A[i,i]\n",
    "        #...and insert into x\n",
    "        x[i]=tmp[1]\n",
    "    end\n",
    "    return x\n",
    "end\n",
    "\n",
    "#Generate response vector\n",
    "b=float(rand(1:10,n))\n",
    "\n",
    "#Solve for y\n",
    "y=fsub(L,b)\n",
    "\n",
    "#Solve for x\n",
    "x=bsub(U,y)\n",
    "\n",
    "println(\"Ly=b:\",csvec(L*y,b))\n",
    "println(\"Ux=y:\",csvec(U*x,y))\n",
    "println(\"Ax=b:\",csvec(A*x,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we are well placed to capture an LU solution function all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ax=b:true\n"
     ]
    }
   ],
   "source": [
    "function LU_solve(A,b)\n",
    "    #Factor A\n",
    "    L,U=LU_fact(A)\n",
    "    #Solve for y\n",
    "    y=fsub(L,b)\n",
    "    #Solve for x\n",
    "    x=bsub(U,y)\n",
    "    return x\n",
    "end\n",
    "\n",
    "#Generate a matrix of random positive integers\n",
    "A=float(rand(1:10,n,n))\n",
    "\n",
    "#Generate response vector\n",
    "b=float(rand(1:10,n))\n",
    "\n",
    "#Solve for x\n",
    "x=LU_solve(A,b)\n",
    "\n",
    "println(\"Ax=b:\",csvec(A*x,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Cholesky Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LU decomposition is a handy method to be sure (and general), but it can be quite expensive computationally.  When the properties of a matrix permit, we can use more efficient methods.  Specifically, when the square matrix $A$ is [Hermitian](https://en.wikipedia.org/wiki/Hermitian_matrix) [positive definite](https://en.wikipedia.org/wiki/Positive-definite_matrix), we can use [Cholesky factorization](https://en.wikipedia.org/wiki/Cholesky_decomposition).\n",
    "\n",
    "Let's unpack this a bit:\n",
    "\n",
    "+ A Hermitian matrix is one in which symmetric entries are complex conjugates of each other (that is $a_{ij} = \\overline{a_{ji}}$).  Complex conjugates are just numbers that have equivalent real parts and equal but opposite imaginary parts (e.g. 4 & 4, or 4+2$i$ & 4-2$i$).  Symmetric real matrices, therefore, are a subset.\n",
    "+ A positive, definite matrix is any Hermitian $n \\times n$ matrix $M$ for which $z^TMz$ is a positive scalar ($z$ being any real or complex non-zero column vector of length $n$).  [Intuitively](http://math.stackexchange.com/questions/9758/intuitive-explanation-of-a-positive-semidefinite-matrix), any vector multiplied by a positive definite matrix cannot have it's angle changed by more than 90 degrees.  It is analogous to multiplying a real variable by a positive number.  It can stretch or contract the number, but it cannot reflect it about the origin.\n",
    "\n",
    "The actual Cholesky decomposition is as follows:\n",
    "\n",
    "$$A=U^TU$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "a_{31} & a_{32} & a_{33} \\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "u_{11} & 0 & 0 \\\\\n",
    "u_{12} & u_{22} & 0 \\\\\n",
    "u_{13} & u_{23} & u_{33} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "u_{11} & u_{12} & u_{13} \\\\\n",
    "0 & u_{22} & u_{23} \\\\\n",
    "0 & 0 & u_{33} \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "where $U$ is upper triangular.  The matrix view makes it easier to see what's happening.  The value of $a_{11}$ is straightforward since there is only single value in the column of $U$ and a single value in the row of its transpose:\n",
    "\n",
    "$$a_{11}=u_{11}u_{11}+0+0$$\n",
    "$$\\rightarrow u_{11} = +\\sqrt{a_{11}}$$\n",
    "\n",
    "In fact, once we have $u_{11}$ the entire first row can be calculated in a similar manner.\n",
    "\n",
    "$$a_{1j}=u_{11}u_{1j}+0+0$$\n",
    "$$\\rightarrow u_{1j} = \\frac{a_{1j}}{u_{11}}$$\n",
    "\n",
    "Once the first row of $U$ is known, we can compute $u_{22}$ and use those first row values in to the second row equations:\n",
    "\n",
    "$$a_{22}=u_{12}^2+u_{22}^2$$\n",
    "$$\\rightarrow u_{22}=+\\sqrt{a_{22} - u_{12}^2}$$\n",
    "$$\\rightarrow u_{2j} = \\frac{a_{2j} - u_{12}u_{1j}}{u_{22}}$$\n",
    "\n",
    "The same can be done with the third row.\n",
    "\n",
    "$$a_{33}=u_{13}^2+u_{23}^2+u_{33}^2$$\n",
    "$$\\rightarrow u_{33}=+\\sqrt{a_{33} - u_{13}^2 - u_{23}^2}$$\n",
    "$$\\rightarrow u_{3j} = \\frac{a_{3j} - u_{13}u_{1j} - u_{23}u_{2j}}{u_{33}}$$\n",
    "\n",
    "The pattern emerging here is a two-step operation for each row.\n",
    "\n",
    "1. $$u_{nn} = +\\sqrt{a_{nn} - \\sum_{i=1}^{n-1} u_{in}^2}$$\n",
    "2. $$u_{nj} = \\frac{a_{nj} - \\sum_{i=1}^{n-1} u_{in}u_{ij}}{u_{nn}}$$\n",
    "\n",
    "\n",
    "These two formulae allow us to completely construct the Cholesky factor $U$.  Once this decomposition has been performed, we proceed in much the same way as we did with the LU factorization.\n",
    "\n",
    "$$Ax = U^TUx = U^T(Ux) = U^Ty = b$$\n",
    "\n",
    "$$Ux = y$$\n",
    "\n",
    "Now that we have seen the Cholesky algorithm, take note of its vulnerability.  Any time the value under the square root in the calculation of $u_{nn}$ is negative or zero, the algorithm fails.  If it is negative, we end up with a complex number.  If it is zero, unique solutions are no longer ensured.  This is why we know the starting matrix $A$ must be positive definite, and conversely, why Cholesky decomposition is a valid test for positive definiteness.  \n",
    "\n",
    "Let's go ahead and write a function that performs Cholesky decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chol_factor (generic function with 1 method)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function chol_factor(A)\n",
    "    #Capture extent of A row dimension\n",
    "    row_n=size(A)[1]\n",
    "    #Generate repository for Cholesky factor    \n",
    "    U=zeros(row_n,row_n)\n",
    "    #For each row...\n",
    "    for n=1:row_n\n",
    "        #...calculate square of the diagonal element of U...\n",
    "        diag2=A[n,n]-dot(U[1:n-1,n],U[1:n-1,n])\n",
    "        #...assign the diagonal element of U...\n",
    "        U[n,n]=sqrt(diag2)\n",
    "        #...and for each column...\n",
    "        for j=1:row_n\n",
    "            #...if we are above the diagonal...\n",
    "            if n<j\n",
    "                #...calculate and assign the upper triangular element of U....\n",
    "                U[n,j]=(A[n,j] - dot(U[1:n-1,n],U[1:n-1,j]))/U[n,n]\n",
    "            else\n",
    "                #...otherwise leave it as zero...\n",
    "                skip\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return U\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, Cholesky decomposition only works on hermitian positive definite matrices.  This [SE answer](http://math.stackexchange.com/questions/357980/matlab-code-for-generating-random-symmetric-positive-definite-matrix) provides an efficient method of generating such matrices.  First, generate a symmetric matrix by adding a random matrix $A$ to its transpose $A^T$.  Then, impose diagonal dominance by adding a diagonal matrix whose elements equal the extent of the either matrix dimension.  Diagonal dominance is given when the diagonal elements exceed the sum of all off diagonal elements (in both the row and column dimensions).\n",
    "\n",
    "$$|A_ii| > \\sum_{i=1}^n |A_ij| \\quad\\forall i \\neq j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5x5 Array{Float64,2}:\n",
       " 5.53383   0.780937  0.922055  0.450563  0.663167\n",
       " 0.780937  5.3567    1.15866   0.251386  0.910032\n",
       " 0.922055  1.15866   5.58419   0.24532   1.15904 \n",
       " 0.450563  0.251386  0.24532   6.22675   1.46227 \n",
       " 0.663167  0.910032  1.15904   1.46227   5.72885 "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function hpd_mat(n)\n",
    "    #Generate random matrix\n",
    "    A=float(rand(n,n))\n",
    "    #Use it to construct a symmetric matrix\n",
    "    A+=transpose(A)\n",
    "    #Impose diagonal dominance\n",
    "    A+=n*eye(n)\n",
    "    return A\n",
    "end\n",
    "\n",
    "test_hpd=hpd_mat(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, this doesn't ensure a positive definiteness with probability 1, but it's close enough to test our function.  If we wanted to ensure an HPD matrix, we could use the fact that such matrices must have all positive eigenvalues.  Meh..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5x5 Array{Float64,2}:\n",
       " 2.35241  0.331973  0.391962  0.191533   0.28191 \n",
       " 0.0      2.29052   0.449039  0.0819912  0.356445\n",
       " 0.0      0.0       2.28668   0.0583503  0.388546\n",
       " 0.0      0.0       0.0       2.48595    0.545619\n",
       " 0.0      0.0       0.0       0.0        2.25248 "
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chol_factor(test_hpd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, does our function work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U'U=A:true\n"
     ]
    }
   ],
   "source": [
    "#Generate HPD matrix\n",
    "A=hpd_mat(n)\n",
    "\n",
    "#Generate Cholesky factor\n",
    "U=chol_factor(A)\n",
    "\n",
    "println(\"U'U=A:\",csvec(transpose(U)*U,A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another situation which can arise is the need to find solutions when $A$ is both large and sparse.  Using Gaussian elimination in this instance leads to the evaluation of a lot of calculations full of zeros, which is not all that useful since those components drop out anyway.  In this situation, a numerical solution can be used to get a solution vector of arbitrary precision.  Consider the following system:\n",
    "\n",
    "$$Ax=b$$\n",
    "\n",
    "We can expand this equation via the inclusion of an additional term $Qx$.\n",
    "\n",
    "$$Ax + Qx = b + Qx$$\n",
    "$$Qx = b + Qx - Ax$$\n",
    "$$Qx = b + (Q-A)x$$\n",
    "$$x = Q^{-1}b + (I - Q^{-1}A)x$$\n",
    "\n",
    "This expression is equivalent to the original version, but now $x$ is on both sides.  Moreover, it looks eerily familiar.  Behold!\n",
    "\n",
    "$$x_{n+1} = x_0 + \\frac{f(x_0)}{f(x_0)'}$$\n",
    "\n",
    "That's right, its the old reliable [Newton-Rhapson](https://en.wikipedia.org/wiki/Newton%27s_method) opti algorithm.  One could also go with the classic equation of a line.\n",
    "\n",
    "$$y = b + mx$$\n",
    "$$\\rightarrow x_1 = x_0 + \\frac{dy}{dx}x$$\n",
    "\n",
    "Anyway, you get the gist.  We are starting at one location, and using the product of $\\delta x$ and the gradient to calculate the next number.  In the Newton context, once that gradient flattens out, we are hitting some inflection point and the search slows to a crawl (or stops entirely).  Within that context, we can add some notation to our iterative expression to make it clearer.\n",
    "\n",
    "$$x^{(k+1)} \\leftarrow Q^{-1}b + (I - Q^{-1}A)x^{(k)}$$\n",
    "\n",
    "The *splitting matrix* $Q$ must satisfy two conditions:\n",
    "\n",
    "1. $Q^{-1}b$ and $Q^{-1}A$ must be easy to compute, which is ensured if $Q$ is either diagonal or triangular.\n",
    "2. We want to converge quickly, which is guaranteed from any value of $|| I-Q^{-1}A|| < 1$.  Indeed, the smaller $|| I-Q^{-1}A||$ gets, the faster we converge.\n",
    "\n",
    "The two most popular methods for this approach are the [Gauss-Jacobi](https://en.wikipedia.org/wiki/Jacobi_method) and [Gauss-Seidel](https://en.wikipedia.org/wiki/Gauss%E2%80%93Seidel_method) methods.  The former uses a diagonal $Q$ that consists of the diagonal elements of $A$.  The latter uses an upper triangular $Q$ formed from the upper triangular elements of $A$.  *Both are guaranteed to converge from any starting value of A is diagonally dominant.*\n",
    "\n",
    "Let's design a function that utilizes the Gauss-Jacobi method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function iter_jacobi(A,b,x0)\n",
    "    #Generate Q\n",
    "    Q=diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5x5 Array{Float64,2}:\n",
       " 5.13443   0.618863  0.832646  0.612594  0.410212\n",
       " 0.618863  5.31728   0.562866  0.458291  1.43199 \n",
       " 0.832646  0.562866  6.48766   0.915645  0.746428\n",
       " 0.612594  0.458291  0.915645  5.30954   1.58603 \n",
       " 0.410212  1.43199   0.746428  1.58603   5.56162 "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "DimensionMismatch(\"*\")\nwhile loading In[208], in expression starting on line 1",
     "output_type": "error",
     "traceback": [
      "DimensionMismatch(\"*\")\nwhile loading In[208], in expression starting on line 1",
      "",
      " in gemm_wrapper! at linalg/matmul.jl:270",
      " in * at linalg/matmul.jl:74"
     ]
    }
   ],
   "source": [
    "diag(A)*eye(size(A)[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.10",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
