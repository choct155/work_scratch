{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![QE](https://avatars3.githubusercontent.com/u/8703060?v=3&s=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra in Julia "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just an intro to the linear algebra tools that will be used later in the QE tutorials.  There is overlap with some of the preceding intro to Julia walkthroughs.  Some this material will be skipped.\n",
    "\n",
    "To start, the inner (or dot) product is defined as follows:\n",
    "\n",
    "$x'y := \\sum_{i=1}^n x_i y_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [10,5,5,1,4,5,1,8,1,8]\n",
      "y: [10,9,5,3,7,6,2,3,9,5]\n",
      "x ̇⋅ y: 306\n"
     ]
    }
   ],
   "source": [
    "#Define two arrays\n",
    "x=rand(1:10,10)\n",
    "y=rand(1:10,10)\n",
    "\n",
    "println(string(\"x: \",x))\n",
    "println(string(\"y: \",y))\n",
    "println(string(\"x ̇⋅ y: \",dot(x,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *norm* of the vector is just the distance from the origin.  It can be found easily with the aid of the Pythagorean Theorem.\n",
    "\n",
    "$||x|| := \\sqrt{x'x} := (\\sum_{i=1}^n x_i^2)^{\\frac{1}{2}}$\n",
    "\n",
    "The distance between to point $x$ and $y$ is just $||x-y||$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.944358444926362\n",
      "17.944358444926362\n"
     ]
    }
   ],
   "source": [
    "#Use intrinsic norm function\n",
    "println(norm(x))\n",
    "\n",
    "#Check with manual calculation\n",
    "println(sqrt(sum(x.^2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of vectors $A$, the span of $A$ is the set of all linear combinations of the vectors in $A$.  One may think of it as the infinite hyperplane in the given dimensions.  For example, observe the span of $A = \\{a_1,a_2\\}$ in $\\mathbb{R}^3$..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2Dspan](http://quant-econ.net/_images/3dvec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we instead defined $A$ as $A = \\{e_1,e_2,e_3\\}$ where the three vectors are the *canonical basis vectors* of $\\mathbb{R}^3$, the span would be all of three dimensional space.  However, if $A = \\{e_1,e_2,e_1+e_2\\}$, the third dimension is unused.  Therefore, the span is again just a plane.  To see why this is true analytically, consider the following vectors:\n",
    "\n",
    "$e_1 = \\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix},$\n",
    "$e_2 = \\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix},$\n",
    "$e_1 + e_2 = \\begin{bmatrix}1\\\\1\\\\0\\end{bmatrix}$\n",
    "\n",
    "With these inputs, we have the following matrix...\n",
    "\n",
    "$A = \\begin{bmatrix}\n",
    "     1 & 0 & 1\\\\\n",
    "     0 & 1 & 1\\\\\n",
    "     0 & 0 & 0\\\\\n",
    "     \\end{bmatrix}$  \n",
    "  \n",
    "...and we must solve for $y = (y_1, y_2, y_3)$.  Using $c_n$ as the coefficient variables, we are faced with the following system of equations:\n",
    "\n",
    "$c_1*(1) + c_2*(0) + c_3*(1) = y_1 = c_1 + c_3\\\\\n",
    "c_1*(0) + c_2*(1) + c_3*(1) = y_2 = c_2 + c_3\\\\\n",
    "c_1*(0) + c_2*(0) + c_3*(0) = y_3 = 0$\n",
    "\n",
    "As can be seen, $y_3=0$, so the vector space does not span all of $\\mathbb{R}^3$.  Rather, it spans $\\mathbb{R}^2$.  This concept of span is one useful path to the idea of linear dependence.  Vectors that do not extend the span of a vector space are linear combinations of the vectors that do.  $e_3$ fits this description, and indeed it was explicitly defined as a linear combination of $e_1$ and $e_2$.  If we were add a vector, $a_3$ to the system depicted in Figure 1, it must lie off the plane formed by the span of $A = \\{a_1,a_2\\}$ to keep the vector space linearly independent.  If $a_3$ lies on that plane, the vector space becomes linearly dependent.  To use terms that resonate more with me, for a new vector to be linearly independent of the existing vectors in a space, it must *extend the dimensionality of the span*.\n",
    "\n",
    "A corollary to this relationship between spans and linear independence is the uniqueness of each linear combination of the vectors in the space.  In other words, if $A$ is linearly independent and $y=\\beta_1 a_1 + ... \\beta_k a_k$, there is no sequence of weights $\\gamma_1 + .... \\gamma_k$ that can produce the same vector $y$.\n",
    "\n",
    "As we well know, the multiplication of matrices just generalizes the inner (or dot) product operation.  In Julia, matrices are straightforward to create.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2\n",
      " 3 4]\n",
      "[1 2\n",
      " 3 4]\n",
      "[1 2\n",
      " 3 4]\n",
      "(2,2)\n"
     ]
    }
   ],
   "source": [
    "A=[1 2\n",
    "   3 4]\n",
    "\n",
    "println(A)\n",
    "\n",
    "B=[1 2;3 4]\n",
    "println(B)\n",
    "\n",
    "C=reshape([1 2 3 4],(2,2))' #Note the transpose\n",
    "println(C)\n",
    "println(size(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication (inner product) leverages the `*` operator, while element-wise multiplication uses the `.*` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 10\n",
      " 15 22]\n",
      "[1 4\n",
      " 9 16]\n"
     ]
    }
   ],
   "source": [
    "println(A*B)\n",
    "println(A.*B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have the following system of equations:  $y=Ax$.  This would be akin to our earlier system...\n",
    "\n",
    "$\\underbrace{\\begin{bmatrix} 1 & 0 & 1\\\\ 0 & 1 & 1\\\\ 0 & 0 & 0 \\end{bmatrix}}_{A} * \\underbrace{\\begin{bmatrix} c_1 \\\\ c_2 \\\\ c_3 \\end{bmatrix}}_{c} = \\underbrace{\\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{bmatrix}}_{y}$  \n",
    "\n",
    "...except we swapped $x$ for $c$.  A natural question would be, given $A$ and $y$, can we solve for $x$?  This is only possible if $y$ is in the range of possible values given the linear combinations in $A$.  Said differently, is $y$ in the range of $f(x)=Ax$?  In this context, the range is the span of the linearly independent subspace of $A$. In other words, if $y \\in \\mathbb{R}^n$ and the span of $f(x)=Ax$ is $\\mathbb{R}^n$ then a solution vector $x$ will exist.  Even better, it will be a unique solution.\n",
    "\n",
    "When the number of rows $n$ in $A$ exceeds the number of columns $k$, existence of a solution is doubtful.  Why is that?  If $A$ is $n \\times k$, then $y$ is $n \\times 1$ and $x$ is $k \\times 1$.  In other words, $y \\in \\mathbb{R}^n$ while $x \\in \\mathbb{R}^k$, so the span of possible solutions is of lower dimensionality than the sought after vector.  Imagine trying to use a plane for the solution when that vector can be anywhere in 3D space.  In this event, the \"solution\" is an approximation seeking to minimize $||y-Ax||$.\n",
    "\n",
    "If the number of rows $n$ in $A$ is less than the number of columns $k$, that means the number of equations ($n$) is less than the number of unknowns ($k$).  Existence may still be preserved, but it won't be unique.  Many $x$ vectors will provide a solution.\n",
    "\n",
    "How might we play with linear systems in Julia?  Suppose we have a simple 2x2 matrix, and a column vector of ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0 2.0\n",
      " 3.0 4.0]\n",
      "[1.0\n",
      " 1.0]\n"
     ]
    }
   ],
   "source": [
    "#Generate 2x2 matrix\n",
    "A=reshape(linspace(1,4,4),(2,2))'\n",
    "println(A)\n",
    "\n",
    "#Generate a column vector of ones\n",
    "y=ones(2,1)\n",
    "println(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Basic operations on the matrices are straight forward.  We can easily solve for $x$ in the system $y=Ax$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we invert this matrix?  You bet your ass we can.  The determinant is -2.0\n",
      "\n",
      "The solution is x = [-0.9999999999999998\n",
      " 0.9999999999999999].\n",
      "\n",
      "That solution better get us back to y if we premultiply by A: [1.0\n",
      " 1.0000000000000004]\n"
     ]
    }
   ],
   "source": [
    "#Capture determinant of A\n",
    "detr=det(A)\n",
    "\n",
    "if detr != 0\n",
    "    println(\"Can we invert this matrix?  You bet your ass we can.  The determinant is $detr\")\n",
    "else\n",
    "    println(\"The determinant is $detr, so matrix is singular!  Womp womp...\")\n",
    "end\n",
    "\n",
    "#Capture inverse of A\n",
    "inv_A=inv(A)\n",
    "\n",
    "#Calculate x\n",
    "x=inv_A*y\n",
    "println(\"\\nThe solution is x = $x.\")\n",
    "\n",
    "#Check solution\n",
    "y_calc=A*x\n",
    "println(\"\\nThat solution better get us back to y if we premultiply by A: $y_calc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at [eigenvalues](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors).  In my opinion, this is one of the most horrifically explained concepts I have had the pleasure of coming across.  They are used in an absolutely remarkable number of ways, and almost without fail, there is no explanation for why it was a good idea to do so.  To be precise, the justification is usually along the following lines: \"...and the eigenvalues are $X$, so naturally, $Y$.\"  You may have noticed, that this is not a justification, just a random use case.\n",
    "\n",
    "When one finds an explanation of what eigenvalues *are*, it generally comes in the following form.\n",
    "\n",
    "+ Let $A$ be an $n \\times n$ matrix;\n",
    "+ Let $v$ be an $n \\times 1$ vector;\n",
    "+ The eigenvalue $\\lambda$ is that scalar value that preserves the following relation: $Av = \\lambda v$.\n",
    "+ That is, it is a value that captures the same linear scaling of the eigenvector as $A$.\n",
    "\n",
    "Is it just me or does this explanation just make it seem like some random mathematical novelty?  It certainly gives me no sense whatsoever as to why it is useful.  So the question is, what do eigenvalues truly mean?  What is the underlying logic that drives their significance in linear algebra?  Why did someone bother to figure this out?  Let us take a moment to explore this in a way that (hopefully) provides some real comprehension.\n",
    "\n",
    "Suppose you have a collection of vectors defining a space in $\\mathbb{R}^n$.  This collection of vectors can be represented as a matrix $A$.  This matrix is a change agent, in that it can be used to linearly transform a vector of choosing.  Why would we do such a thing?  In fact, we make changes of this type all the time.  Another way of describing the application of the transform embodied in $A$ on some vector construct ($x$) is to say that we are passing $x$ into a function.  That is, $y=Ax$ is quite analogous to the expression $y=f(x)$.  In both instances, we are mapping values from the domain of $x$ (say $\\mathbb{R}^k$) to the codomain contained within the domain of $y$ (say $\\mathbb{R}^n$).  The $f(x)$ notation leaves open the specific form of this mapping, but our use of $Ax$ suggests a narrowing of possibilities down to that which can be considered a linear transform.  For example, let's say we have this vector $x=\\begin{bmatrix}1\\\\2\\end{bmatrix}$ and a transformation matrix $A=\\begin{bmatrix} 1 & 2\\\\3 & 4\\end{bmatrix}$.  We can determine the image of $x$ in the domain of $y$ by way of $A$:\n",
    "\n",
    "$$f(x) = \\underbrace{\\begin{bmatrix} 1 & 2\\\\3 & 4\\end{bmatrix}}_A \\underbrace{\\begin{bmatrix}1\\\\2\\end{bmatrix}}_x = \\underbrace{\\begin{bmatrix}5\\\\11\\end{bmatrix}}_y$$\n",
    "\n",
    "If we were to graph these, we would see that applying $A$ to $x$ has, in fact, changed the direction of the original vector.  Instead of $x$ is 2, while the slope of $y$ is $\\frac{11}{5}$.  It turns out, however, that a given transformation matrix $A$ does not change the direction of all vectors.  Some vectors continue to feature the same [span](https://en.wikipedia.org/wiki/Linear_span), and are altered only in their length, if at all.  (Note that the length can go negative.  That is, it can point the other way.)  These vectors are characteristic to a given matrix $A$, and they are known as eigenvectors.  To determine which vectors have this property for a given matrix $A$, one must know the characteristic roots of $A$, also known as eigenvalues.  Once we have this info, we know that a subset of the vectors in a subspace will remain reliably in the same direction under the transform.  \n",
    "\n",
    "![eigen](https://upload.wikimedia.org/wikipedia/commons/0/06/Eigenvectors.gif)\n",
    "\n",
    "Why is this useful?  For starters, because they don't change direction under transformation, eigenvectors are useful transformation-specific \"axes\" that can be used to orthogonalize data.  This is, for example, what is happening when we perform [principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis).  We are shifting the basis to line up with linearly independent eigenvectors of the system.  They are also useful for decomposing a given matrix $A$.  Note how $Ax=\\lambda x \\rightarrow A=x^{-1}\\lambda x$.  Since $lambda$ is a diagonal matrix (of eigenvalues) it makes it really easy to calculate $A^n$ for any $n$.  Finally, by distilling a linear transformation $A$ down to a scalar multiplier $\\lambda$ there are many problems that can be made easier.  Instead of dealing with all of the variables that likely appear in multiple equations in the system, certain aspects of the problem can deal with just the eigenvalues.  (See a great [SO post](http://math.stackexchange.com/questions/23312/what-is-the-importance-of-eigenvalues-eigenvectors) on this.)  There are a million other uses I don't really know much about, but trust me, the list is extensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate plot object\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.10",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
